
# ğŸ§  Self-Reflection Agentic RAG Backend

A production-ready **Agentic RAG (Retrieval-Augmented Generation)** backend built with **Node.js + TypeScript**, designed to provide **autonomous reasoning, memory, and self-improving AI responses**.  
It combines **retrieval, self-reflection, memory storage, and planning** to deliver more accurate, contextual, and human-like answers over time.

---

## ğŸš€ Overview

This backend goes beyond simple RAG by adding **agentic intelligence** and **self-reflection** capabilities:

- ğŸ¤– **Agentic Planning** â€“ Breaks complex queries into subtasks and executes them step-by-step.  
- ğŸ§  **Self-Reflection** â€“ After every response, the agent evaluates quality, identifies gaps, and refines future answers.  
- ğŸ“š **Memory System** â€“ Stores both short-term (conversation) and long-term (vector) memory for continuous learning.  
- ğŸ” **Retrieval-Augmented Generation** â€“ Combines semantic search with LLM reasoning for grounded, factual results.  
- ğŸ§° **Extensible Tools** â€“ Modular toolset for searching, reasoning, summarizing, and more.

---

## ğŸ—‚ï¸ Project Structure

```
agentic-rag-backend/
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ agent/               # Core agent logic
â”‚  â”‚   â”œâ”€ planner.ts       # Breaks queries into subtasks
â”‚  â”‚   â”œâ”€ reflection.ts    # Self-reflection loop
â”‚  â”‚   â”œâ”€ executor.ts      # Tool orchestration
â”‚  â”‚   â””â”€ memory.ts        # Short-term & long-term memory
â”‚  â”‚
â”‚  â”œâ”€ retrieval/           # Vector store & retrieval logic
â”‚  â”‚   â”œâ”€ embedder.ts      # Embedding generation
â”‚  â”‚   â”œâ”€ vectorStore.ts   # FAISS / SQLite storage
â”‚  â”‚   â””â”€ retriever.ts     # Semantic search
â”‚  â”‚
â”‚  â”œâ”€ api/                 # Express REST API
â”‚  â”‚   â””â”€ routes.ts        # `/rag/query`, `/ingest`, etc.
â”‚  â”‚
â”‚  â”œâ”€ utils/               # Helpers & config
â”‚  â”‚   â””â”€ logger.ts
â”‚  â”‚
â”‚  â””â”€ server.ts            # Entry point
â”‚
â”œâ”€ .env.example            # Environment variable template
â”œâ”€ package.json
â”œâ”€ tsconfig.json
â””â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/agentic-rag-backend.git
cd agentic-rag-backend
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Environment

Create a `.env` file from the example:

```bash
cp .env.example .env
```

Set your keys and configs:
```
OPENAI_API_KEY=your_openai_key
EMBEDDING_MODEL=text-embedding-3-small
VECTOR_DB_PATH=./data/vectorstore.db
```

---

## â–¶ï¸ Running the Server

### Development
```bash
npx tsx initMemory
```
```bash
npx tsx server
```
### Production
```bash
npm run build
npm start
```

Server will run on:  
ğŸ‘‰ `http://localhost:5000`

---

## ğŸ”¥ API Endpoints

### ğŸ“¥ Ingest Document  
**POST** `/api/ingest`  
Upload and embed documents into the vector database.

**Body:**
```json
{
  "text": "Paste or upload your document content here."
}
```

---

### ğŸ¤– Query Agent  
**POST** `/api/rag/query`  
Ask a question â€” the agent retrieves, reasons, reflects, and responds.

**Body:**
```json
{
  "query": "What are the main challenges in quantum computing?"
}
```

**Response:**
```json
{
  "answer": "The main challenges include error correction, qubit stability, and scalability...",
  "reflection": "The initial answer lacked detail on scalability. Future responses will elaborate further.",
  "sources": []
}
```

---

## ğŸ§  Core Architecture

### 1. Retrieval Layer  
- Ingests documents and converts them into embeddings  
- Uses FAISS / SQLite for semantic vector search  

### 2. Agent Core  
- Plans multi-step tasks  
- Calls appropriate tools (search, summarization, etc.)  
- Executes reasoning chain

### 3. Self-Reflection Loop  
- Analyzes its own answers  
- Detects weaknesses or missing info  
- Stores â€œinsightsâ€ into memory for future responses

### 4. Memory System  
- **Short-Term Memory:** Session-based context  
- **Long-Term Memory:** Persistent vector database  
- **Reflections:** Stored as metadata to guide future behavior

---

## ğŸ§° Tech Stack

| Component              | Technology                      |
|------------------------|-------------------------------|
| Server                | Node.js + Express.js          |
| Language              | TypeScript                    |
| Vector DB            | FAISS / SQLite               |
| LLM Integration      | OpenAI / Groq / Ollama APIs  |
| Memory               | Custom embedding store        |
| Self-Reflection      | Custom reasoning pipeline     |

---

## ğŸ§  Example Flow

1. ğŸ“„ **Ingest:** Upload docs â†’ embedded â†’ vector DB  
2. ğŸ” **Retrieve:** Agent searches relevant chunks  
3. ğŸ§© **Plan:** Planner splits complex question into subtasks  
4. ğŸ§  **Reflect:** Checks output quality â†’ improves reasoning  
5. ğŸ“š **Store:** Reflection & context saved for future sessions  

---

## ğŸ§ª Roadmap

- [ ] Multi-user isolated memory  
- [ ] Autonomous multi-step reasoning  
- [ ] Web UI chat with reflection view  
- [ ] Tool-calling for external APIs  
- [ ] Local LLM integration (Ollama / GGUF)

---

## ğŸ¤ Contributing

Contributions are welcome!  
1. Fork the repo  
2. Create a feature branch: `git checkout -b feature/new-feature`  
3. Commit changes: `git commit -m "Add new feature"`  
4. Push: `git push origin feature/new-feature`  
5. Open a PR ğŸš€

---

## ğŸ“œ License

MIT License Â© 2025 [Your Name / Sysnut Technologies]

---

## ğŸŒŸ Acknowledgements

- Inspired by LangChain Agents, ReAct, and Self-Reflective AI research.  
- Built for production-grade RAG applications with continuous learning.
